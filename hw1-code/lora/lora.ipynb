{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c567ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `shasha` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `shasha`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hf_xxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49878c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa233a391454b69baa1176765e22676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3b51e3722b4d079faeb834eb4b0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c70db9c9a54fcbb83639973facae9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86dd912960649ea806691ba5ffe3d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd53e5263615434ea20969dd26564292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef3b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的推理函数\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, # 避免模型复读，默认值为1.0\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56e18a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this movie because it is so funny and I am sure that my friends will enjoy too']\n"
     ]
    }
   ],
   "source": [
    "# 测试一下这个推理函数\n",
    "input_sentences = tokenizer(\"I love this movie because\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afcc3d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7307bfa737c04cc4810f724ccb71b673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5320b203b0a1492f8ac9973d1755f839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485f7e3f7f7d4e4cbc2afc9cda1bb633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = \"noob123/imdb_review_3000\"\n",
    "\n",
    "#Create the Dataset to create prompts.\n",
    "data = load_dataset(dataset)\n",
    "data = data.map(lambda samples: tokenizer(samples['review']), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "\n",
    "train_sample = train_sample.remove_columns('sentiment')\n",
    "\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f9a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': [\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"], 'input_ids': [[17785, 461, 368, 3390, 17953, 525, 1809, 28950, 861, 4731, 82469, 3370, 404, 75671, 63966, 30845, 722, 1877, 51478, 17, 12941, 1306, 4548, 15, 661, 1119, 632, 24308, 3595, 36643, 1002, 1074, 17, 27069, 154878, 10398, 2175, 3968, 11736, 861, 98581, 1074, 3638, 75671, 1620, 3776, 36213, 25207, 530, 447, 3326, 107029, 386, 125347, 461, 21415, 15, 2131, 1907, 361, 4548, 1485, 368, 14679, 55536, 17, 86036, 1074, 15, 1119, 632, 1130, 267, 6460, 613, 368, 142593, 23724, 376, 791, 5884, 356, 17, 3904, 6460, 141478, 654, 5755, 7231, 1002, 62262, 427, 51667, 15, 7025, 791, 21415, 17, 41969, 632, 12587, 15214, 15, 361, 368, 95460, 2971, 461, 368, 14679, 17, 27069, 154878, 10398, 3548, 632, 9487, 770, 61, 661, 861, 632, 368, 75221, 3312, 8885, 427, 368, 7890, 87547, 177212, 30333, 14688, 9660, 61993, 1912, 17, 3162, 133680, 44363, 664, 145082, 3733, 14585, 15, 660, 45173, 13773, 461, 368, 22691, 4618, 1728, 368, 13953, 1542, 43810, 178683, 530, 10844, 361, 12548, 15, 1427, 115777, 632, 1130, 6426, 664, 368, 25169, 17, 5749, 14585, 632, 10863, 427, 7112, 566, 3375, 92, 703, 15, 131914, 15, 46300, 553, 296, 15, 10358, 9209, 15, 144288, 15, 65965, 703, 15, 67026, 530, 3172, 3250, 1515, 3359, 7145, 1336, 15, 14905, 960, 7708, 15, 140032, 34801, 17691, 3164, 530, 1804, 21239, 72666, 1306, 11705, 8372, 14723, 17, 27069, 154878, 10398, 44, 3276, 5894, 368, 4291, 57709, 461, 368, 6460, 632, 11594, 427, 368, 5919, 861, 718, 22108, 4618, 3390, 17082, 44657, 142502, 17, 5070, 1587, 26489, 65713, 148724, 613, 125432, 199617, 15, 50827, 85753, 15, 50827, 73190, 1369, 50, 61, 11326, 17968, 10515, 17, 1387, 3968, 63966, 473, 19082, 25338, 98581, 1074, 661, 1427, 294, 74182, 718, 1620, 174123, 15, 473, 45240, 5894, 473, 1620, 29659, 613, 718, 15, 1965, 661, 473, 135986, 3172, 15, 473, 24740, 267, 79761, 613, 75671, 15, 530, 8610, 2995, 7432, 376, 427, 368, 6426, 22064, 461, 148157, 21415, 17, 10366, 3370, 21415, 15, 1965, 204372, 375, 82473, 51478, 141771, 5268, 8722, 722, 16993, 1800, 613, 267, 157045, 15, 216398, 5268, 8722, 22552, 664, 7092, 530, 2213, 14723, 1002, 718, 15, 6355, 33365, 376, 15, 28834, 2591, 216398, 6610, 38796, 3727, 22691, 9705, 7231, 11594, 427, 3808, 31345, 461, 40634, 57626, 791, 22691, 24575, 12, 58613, 386, 75671, 15, 1152, 2742, 17158, 103399, 1002, 3595, 632, 243351, 132762, 3250, 724, 1531, 1320, 1152, 1400, 2213, 361, 17717, 1002, 2632, 32046, 259, 11919, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3934851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4, #As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44bd34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 393,216 || all params: 559,607,808 || trainable%: 0.07026635339584111\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa0803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eec4f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=2,\n",
    "    use_cpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe50c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiaodee-ai\u001b[0m (\u001b[33mmiaode-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/lora/wandb/run-20250613_232618-o20skde2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miaode-ai/huggingface/runs/o20skde2' target=\"_blank\">./peft_lab_outputs</a></strong> to <a href='https://wandb.ai/miaode-ai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miaode-ai/huggingface' target=\"_blank\">https://wandb.ai/miaode-ai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miaode-ai/huggingface/runs/o20skde2' target=\"_blank\">https://wandb.ai/miaode-ai/huggingface/runs/o20skde2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=3.7620725631713867, metrics={'train_runtime': 12.3132, 'train_samples_per_second': 8.121, 'train_steps_per_second': 0.325, 'total_flos': 152578700673024.0, 'train_loss': 3.7620725631713867, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de4828cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e41c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this movie because it is so good. It has a lot of action and drama, but also lots to laugh about! I really enjoyed the ending as well - it\\'s very funny that we get married in such an awkward situation (and then have kids). The storyline was interesting too; there are some great twists throughout which you will probably not be able for long enough before watching again or even hearing from your friends who\\'ve been together since they were young...but that\\'s all right if you\\'re into romance movies like \"The']\n"
     ]
    }
   ],
   "source": [
    "loaded_model = PeftModel.from_pretrained(foundation_model, peft_model_path, is_trainable=False)\n",
    "input_sentences = tokenizer(\"I love this movie because\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
