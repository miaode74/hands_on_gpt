{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49878c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f04d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `shasha` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `shasha`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hf_ZkqzrDxSlmqd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(record):\n",
    "    \"\"\"把 Alpaca 三段字段拼成一条指令 Prompt\"\"\"\n",
    "    if record[\"input\"]:\n",
    "        return (\n",
    "            f\"### Instruction:\\n{record['instruction']}\\n\\n\"\n",
    "            f\"### Input:\\n{record['input']}\\n\\n\"\n",
    "            f\"### Response:\\n{record['output']}\"\n",
    "        )\n",
    "    else:  # 没有 input 的条目 _for alpaca\n",
    "        return (\n",
    "            f\"### Instruction:\\n{record['instruction']}\\n\\n\"\n",
    "            f\"### Response:\\n{record['output']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的推理函数\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, # 避免模型复读，默认值为1.0\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e18a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this movie because it is so funny and I am sure that my friends will enjoy too']\n"
     ]
    }
   ],
   "source": [
    "# 测试一下这个推理函数\n",
    "input_sentences = tokenizer(\"I love this movie because\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e55c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 512        # 你自己决定 —— 通常 256 / 512 / 1024\n",
    "train_on_inputs = False # 常用设置：只让模型学习回答，不回传问句梯度\n",
    "add_eos_token  = True\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # 补 <eos>\n",
    "    if (result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()  # 先默认全部监督\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(dp):\n",
    "    full_prompt = build_prompt(dp)\n",
    "    tokenized = tokenize(full_prompt)\n",
    "\n",
    "    if not train_on_inputs:  # 把问句位置 mask 成 -100\n",
    "        user_prompt = build_prompt({**dp, \"output\": \"\"})  # 只 instruction+input\n",
    "        user_tok = tokenize(user_prompt, add_eos_token=add_eos_token)\n",
    "        n = len(user_tok[\"input_ids\"]) - (1 if add_eos_token else 0)\n",
    "        tokenized[\"labels\"] = [-100] * n + tokenized[\"labels\"][n:]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2f790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8595f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc3d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a7186db424ac3aacdde4bde17c082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Column name ['sentiment'] not in the dataset. Current columns in the dataset: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m alpaca_ds\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# train_sample = alpaca_ds.map(generate_and_tokenize_prompt)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train_sample = data[\"train\"]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m display(train_sample)\n",
      "File \u001b[0;32m/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/datasets/arrow_dataset.py:2222\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2220\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2225\u001b[0m     )\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Column name ['sentiment'] not in the dataset. Current columns in the dataset: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = \"noob123/imdb_review_3000\"\n",
    "alpaca_ds = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "#Create the Dataset to create prompts.\n",
    "# data = load_dataset(dataset)\n",
    "# data = load_dataset(alpaca_ds)\n",
    "\n",
    "# data = data.map(lambda samples: tokenizer(samples['review']), batched=True)\n",
    "# train_sample = data[\"train\"].select(range(50))\n",
    "\n",
    "train_sample = alpaca_ds.map(generate_and_tokenize_prompt)\n",
    "# train_sample = alpaca_ds.map(generate_and_tokenize_prompt)\n",
    "# train_sample = data[\"train\"]\n",
    "\n",
    "\n",
    "# train_sample = train_sample.remove_columns('sentiment')\n",
    "train_sample = train_sample.remove_columns(\n",
    "    [col for col in train_sample.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
    ")\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': [\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"], 'input_ids': [[17785, 461, 368, 3390, 17953, 525, 1809, 28950, 861, 4731, 82469, 3370, 404, 75671, 63966, 30845, 722, 1877, 51478, 17, 12941, 1306, 4548, 15, 661, 1119, 632, 24308, 3595, 36643, 1002, 1074, 17, 27069, 154878, 10398, 2175, 3968, 11736, 861, 98581, 1074, 3638, 75671, 1620, 3776, 36213, 25207, 530, 447, 3326, 107029, 386, 125347, 461, 21415, 15, 2131, 1907, 361, 4548, 1485, 368, 14679, 55536, 17, 86036, 1074, 15, 1119, 632, 1130, 267, 6460, 613, 368, 142593, 23724, 376, 791, 5884, 356, 17, 3904, 6460, 141478, 654, 5755, 7231, 1002, 62262, 427, 51667, 15, 7025, 791, 21415, 17, 41969, 632, 12587, 15214, 15, 361, 368, 95460, 2971, 461, 368, 14679, 17, 27069, 154878, 10398, 3548, 632, 9487, 770, 61, 661, 861, 632, 368, 75221, 3312, 8885, 427, 368, 7890, 87547, 177212, 30333, 14688, 9660, 61993, 1912, 17, 3162, 133680, 44363, 664, 145082, 3733, 14585, 15, 660, 45173, 13773, 461, 368, 22691, 4618, 1728, 368, 13953, 1542, 43810, 178683, 530, 10844, 361, 12548, 15, 1427, 115777, 632, 1130, 6426, 664, 368, 25169, 17, 5749, 14585, 632, 10863, 427, 7112, 566, 3375, 92, 703, 15, 131914, 15, 46300, 553, 296, 15, 10358, 9209, 15, 144288, 15, 65965, 703, 15, 67026, 530, 3172, 3250, 1515, 3359, 7145, 1336, 15, 14905, 960, 7708, 15, 140032, 34801, 17691, 3164, 530, 1804, 21239, 72666, 1306, 11705, 8372, 14723, 17, 27069, 154878, 10398, 44, 3276, 5894, 368, 4291, 57709, 461, 368, 6460, 632, 11594, 427, 368, 5919, 861, 718, 22108, 4618, 3390, 17082, 44657, 142502, 17, 5070, 1587, 26489, 65713, 148724, 613, 125432, 199617, 15, 50827, 85753, 15, 50827, 73190, 1369, 50, 61, 11326, 17968, 10515, 17, 1387, 3968, 63966, 473, 19082, 25338, 98581, 1074, 661, 1427, 294, 74182, 718, 1620, 174123, 15, 473, 45240, 5894, 473, 1620, 29659, 613, 718, 15, 1965, 661, 473, 135986, 3172, 15, 473, 24740, 267, 79761, 613, 75671, 15, 530, 8610, 2995, 7432, 376, 427, 368, 6426, 22064, 461, 148157, 21415, 17, 10366, 3370, 21415, 15, 1965, 204372, 375, 82473, 51478, 141771, 5268, 8722, 722, 16993, 1800, 613, 267, 157045, 15, 216398, 5268, 8722, 22552, 664, 7092, 530, 2213, 14723, 1002, 718, 15, 6355, 33365, 376, 15, 28834, 2591, 216398, 6610, 38796, 3727, 22691, 9705, 7231, 11594, 427, 3808, 31345, 461, 40634, 57626, 791, 22691, 24575, 12, 58613, 386, 75671, 15, 1152, 2742, 17158, 103399, 1002, 3595, 632, 243351, 132762, 3250, 724, 1531, 1320, 1152, 1400, 2213, 361, 17717, 1002, 2632, 32046, 259, 11919, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4, #As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 393,216 || all params: 559,607,808 || trainable%: 0.07026635339584111\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=2,\n",
    "    use_cpu=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"lora-bloomz-peft\",\n",
    "    logging_strategy=\"steps\",       # ✅ 每隔几个 step 记录一次\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiaodee-ai\u001b[0m (\u001b[33mmiaode-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/lora/wandb/run-20250616_135538-qtcnxnnk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miaode-ai/huggingface/runs/qtcnxnnk' target=\"_blank\">lora-bloomz-peft</a></strong> to <a href='https://wandb.ai/miaode-ai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miaode-ai/huggingface' target=\"_blank\">https://wandb.ai/miaode-ai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miaode-ai/huggingface/runs/qtcnxnnk' target=\"_blank\">https://wandb.ai/miaode-ai/huggingface/runs/qtcnxnnk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cpfs/user/boyuan/miniconda3/envs/pretrained/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28/188 00:16 < 01:43, 1.55 it/s, Epoch 0.29/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.723300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/376 02:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.545500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>9.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>9.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>8.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>9.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>9.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>9.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>10.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>10.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>10.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>10.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>10.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>10.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>10.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>10.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>9.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>10.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>9.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>9.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>9.730400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>9.777200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>10.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>10.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>10.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>10.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>9.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>9.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>9.481900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=376, training_loss=9.622473858772441, metrics={'train_runtime': 174.4131, 'train_samples_per_second': 34.39, 'train_steps_per_second': 2.156, 'total_flos': 9405951639846912.0, 'train_loss': 9.622473858772441, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4828cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41c769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I love this movie because the was his he and that in, left to a is man last itself's on as but of by also their had with since work noticeides w war into her him it your you right following for over be made got will has thought.' more most recently its an not never them court or cost way then first special all attention again aroundplace I y kakak instead rather short one house ta tour much she they there-w stless disc McCarthy at un set my star! chart others while quarter intent-m\"]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = PeftModel.from_pretrained(foundation_model, peft_model_path, is_trainable=False)\n",
    "input_sentences = tokenizer(\"I love this movie because\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
