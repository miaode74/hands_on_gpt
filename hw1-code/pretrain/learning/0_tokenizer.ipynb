{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5d51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (decoders,models,normalizers,pre_tokenizers,processors,trainers,Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faed99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e6b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\",\"<s>\",\"</s>\"]\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=256,special_tokens=special_tokens,show_progress=True,initial_alphabet=pre_tokenizers.ByteLevel.alphabet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ebbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import json\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b78f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer():\n",
    "    def read_texts_from_jsonl(file_path):\n",
    "        with open(file_path,'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                yield data['text']\n",
    "    \n",
    "    data_path = \"/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/pretrain/dataset/pretrain_hq.jsonl\"\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    special_tokens = [\"<|endoftext|>\",\"<|im_start|>\",\"<|im_end|>\"]\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=6400,\n",
    "        special_tokens=special_tokens,\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    "    )\n",
    "\n",
    "    texts = read_texts_from_jsonl(data_path)\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    assert tokenizer.token_to_id(\"<|endoftext|>\") == 0\n",
    "    assert tokenizer.token_to_id(\"<|im_start|>\") == 1\n",
    "    assert tokenizer.token_to_id(\"<|im_end|>\") == 2\n",
    "\n",
    "    tokenizer_dir = \"/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/pretrain/models/\"\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "    tokenizer.model.save(tokenizer_dir)\n",
    "\n",
    "    config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False,\n",
    "        \"add_tokens_docoder\":{\n",
    "            \"0\":{\n",
    "                \"content\" : \"<|endoftext|>\",\n",
    "                \"lstrip\" : False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"1\": {\n",
    "                \"content\": \"<|im_start|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"2\": {\n",
    "                \"content\": \"<|im_end|>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "                },\n",
    "    },\n",
    "    \"additional_special_tokens\": [],\n",
    "    \"bos_token\": \"<|im_start|>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"<|im_end|>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 32768,\n",
    "    \"pad_token\": \"<|endoftext|>\",\n",
    "    \"sp_model_kwargs\":{},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{{ '<|im_start|>system\\\\n' + system_message + '<|im_end|>\\\\n' }}{% else %}{{ '<|im_start|>system\\\\nYou are a helpful assistant<|im_end|>\\\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + content + '<|im_end|>\\\\n<|im_start|>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "    }\n",
    "    with open(os.path.join(tokenizer_dir, \"config.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Tokenizer training complete and saved to\", tokenizer_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6922a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tokenizer():\n",
    "    from transformers import AutoTokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/pretrain/models/\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，充满了幽默感。\"},\n",
    "        {\"role\": \"user\", \"content\": \"请问我女朋友又爱吃香菜又爱吃榴莲会发生什么事？\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"那简直是世界上最可爱的女朋友了！她的味蕾一定是个冒险家，敢于挑战所有的味道组合！香菜和榴莲的结合，可能会让她的味觉体验达到巅峰！\"},\n",
    "        {\"role\": \"user\", \"content\": \"你能帮我写一首关于香菜和榴莲的诗吗？\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"香菜飘香榴莲甜，\\n味蕾交织如梦幻。\\n一口清新一口浓，\\n两种风味共此生。\"}\n",
    "    ]\n",
    "\n",
    "    new_prompt = tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "    print(\"New Prompt:\", new_prompt)\n",
    "\n",
    "    actual_vocab_size = len(tokenizer)\n",
    "    print(\"Actual Vocab Size:\", actual_vocab_size)\n",
    "\n",
    "    model_inputs = tokenizer(new_prompt)\n",
    "    print(\"Model Inputs:\", len(model_inputs['input_ids']))\n",
    "\n",
    "    inputs_ids = model_inputs['input_ids']\n",
    "    response = tokenizer.decode(inputs_ids, skip_special_tokens=False)\n",
    "    print(\"If Decoded Response equal? :\", response == new_prompt)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc54acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfe4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67a45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens: 1413103it [03:29, 6751.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 668061022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_tokens_with_tiktoken(jsonl_path, encoder_name=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(encoder_name)\n",
    "    total_tokens = 0\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Counting tokens\"):\n",
    "            data = json.loads(line)\n",
    "            text = data.get(\"text\", \"\")\n",
    "            tokens = enc.encode(text)\n",
    "            total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "# 示例路径\n",
    "jsonl_path = \"/cpfs/user/boyuan/verl_workspace/hw1_code/hw1-code/pretrain/dataset/pretrain_hq.jsonl\"\n",
    "total = count_tokens_with_tiktoken(jsonl_path)\n",
    "print(\"Total tokens:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290fa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
