{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fe0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e11a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "import time\n",
    "from typing import Any, Optional, Tuple, List\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "from transformers import PreTrainedModel \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659ffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        return self.weight * (x.float() * torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps)).type_as(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7aee7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_pos_cis(dim: int, end: int = int(32*1024), theta: float = 1e6):\n",
    "    freq = 1.0 / (theta ** (torch.arange(0,dim,2)[: (dim//2)].float() / dim))\n",
    "\n",
    "    t = torch.arange(end, device=freq.device)\n",
    "    freqs = torch.outer(t,freqs).float()\n",
    "    pos_cis = torch.polar(torch.ones_like(freqs),freqs)\n",
    "    return pos_cis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotray_emb(xq, xk, pos_cis):\n",
    "    def unite_shape(pos_cis,x):\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
    "        shape = [d if i==1 or i == ndim -1 else 1 for i, d in enumerate(x.shape)]\n",
    "        return pos_cis.view(*shape)\n",
    "    \n",
    "    xq_ = troch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1,2))\n",
    "    xk_ = troch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1,2))\n",
    "    pos_cis  = unite_shape(pos_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ee528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,None,:].expand(bs, slen, n_kv_heads, n_rep, head_dim).reshape(bs,slen,n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
