# 1.1 部署align-anything，训练奖励模型（30%）
## 1.1.1 偏好数据集键值转换（10%）
## 1.1.2 训练奖励模型（10%）
## 1.1.3 评测奖励模型（10%）
## 1.1.4 使用奖励模型可视化偏好数据集（10%）


# DPO微调（40%）
## 2.1 使用DPO微调模型（30%）


## 2.1.1 运行DPO微调（15%）


## 2.1.2 评测DPO微调模型（15%）


## 2.2 回答问题（10%）
    从强化学习的角度，DPO是on-policy还是off-policy的，是online的还是offline的，为什么？
    DPO主要针对传统RLHF的哪个方面进行了优化，关键见解是什么？
    DPO和传统RLHF相比有哪些局限性，具体体现在哪些方面？
    现有的研究（KTO，SimPO，ORPO等）主要从哪些方面对DPO进行优化？


# 3. Bonus: 制作一个text-to-text DPO的ipynb文件（50%）

# 4. DPO KTO APO step-DPO 等等之间的区别 然后实现所有版本的 text-to-text.